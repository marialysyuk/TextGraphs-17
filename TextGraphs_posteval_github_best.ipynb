{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35c11a0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/kbqa/TextGraphs/TextGraphs17-shared-task\n"
     ]
    }
   ],
   "source": [
    "! git clone https://github.com/uhh-lt/TextGraphs17-shared-task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3dad561e-ec8c-46b3-994a-f6e0d5c93e03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/kbqa/TextGraphs/TextGraphs17-shared-task\n"
     ]
    }
   ],
   "source": [
    "%cd TextGraphs17-shared-task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1afaee4-329a-460f-a6a6-c9de226e96b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install fuzzywuzzy python-Levenshtein sister"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b53c21e6-0746-4c66-b2ab-9d897c6910ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from fuzzywuzzy import fuzz\n",
    "import ast\n",
    "from Levenshtein import distance as levenshtein_distance\n",
    "import sister\n",
    "import scipy\n",
    "sentence_embedding = sister.MeanEmbedding(lang=\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f62dd78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('data/tsv/test.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "129df670-a2ff-4353-80a2-e5d824525193",
   "metadata": {},
   "outputs": [],
   "source": [
    "konst = pd.read_csv('data/csv/textgraphs_konstruktor_pred.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32a33c90-6e84-4c3c-bf4a-22ef06429c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['correct_path'] = konst['correct_path']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98133118",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_questions_test = test_data.question.unique()\n",
    "\n",
    "unique_questions_dict_test = dict()\n",
    "for i in range(len(unique_questions_test)):\n",
    "    unique_questions_dict_test[unique_questions_test[i]] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6ebd281",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(test_data)):\n",
    "    test_data.loc[i, 'question_id'] = unique_questions_dict_test[test_data.loc[i, 'question']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56c8148-9b21-4d3a-a5e1-38a6233c1496",
   "metadata": {},
   "source": [
    "### Download GPT-4 predictions initial, with rephrased answers and answers descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b309920",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_gpt = pd.read_json(path_or_buf='data/csv/text2graph_test_with_answers.jsonl', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "093576b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_gpt = test_gpt.rename(columns = {'predText': 'pred_gpt'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37f53efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_gpt_gpt4 = pd.read_json(path_or_buf='data/csv/text2graph_test_with_answers_gpt4_nonanswered.jsonl', lines=True)\n",
    "test_gpt_gpt4 = test_gpt_gpt4.rename(columns = {'predText': 'pred_gpt'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4c2ca75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_gpt_gpt3_ids = test_gpt.id.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "32656597",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_gpt_gpt4_all = pd.read_json(path_or_buf='data/csv/text2graph_test_with_answers_gpt4_all.jsonl', lines=True)\n",
    "test_gpt_gpt4_all = test_gpt_gpt4_all.rename(columns = {'predText': 'pred_gpt'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ec191b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_gpt_gpt4_ids = test_gpt_gpt4_all.id.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c4e3133f-04e8-4296-a352-5dee266925a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_gpt_gpt4_rephrased = pd.read_json(path_or_buf='data/csv/text2graph_test_with_answers_gpt4_nonanswered_rephrased.jsonl', lines=True)\n",
    "test_gpt_gpt4_rephrased = test_gpt_gpt4_rephrased.rename(columns = {'predText': 'pred_gpt'})\n",
    "test_gpt_gpt4_rephrased_ids = test_gpt_gpt4_rephrased.id.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "80e2f826-5a0d-4221-9c91-75db89ef1693",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_gpt_gpt4_answerdesc = pd.read_json(path_or_buf='data/csv/text2graph_test_with_answers_gpt4_nonanswered_answerdesc.jsonl', lines=True)\n",
    "test_gpt_gpt4_answerdesc = test_gpt_gpt4_answerdesc.rename(columns = {'predText': 'pred_gpt'})\n",
    "test_gpt_gpt4_answerdesc_ids = test_gpt_gpt4_answerdesc.id.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "53c2df2e-0073-4383-8427-8eeae9434d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_konstruktor = pd.read_json(path_or_buf='data/csv/text2graph_test_with_konstruktor.jsonl', lines=True)\n",
    "test_konstruktor = test_konstruktor.rename(columns = {'predText': 'pred_gpt'})\n",
    "test_konstruktor_ids = test_konstruktor.id.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096da025-9781-4fed-985e-7a526c228ab6",
   "metadata": {},
   "source": [
    "### Check exact match with gpt-4 candidates and candidate answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fe0ec9c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10961/10961 [00:16<00:00, 667.21it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(len(test_data))):\n",
    "    if test_data.loc[i, 'question_id'] in test_gpt_gpt4_ids:\n",
    "        cur_ques_id = test_data.loc[i, 'question_id']\n",
    "        gpt_ind = test_gpt_gpt4_all.query('id == @cur_ques_id')['pred_gpt'].index[0]\n",
    "        test_data.loc[i, 'pred_gpt'] = test_gpt_gpt4_all.query('id == @cur_ques_id')['pred_gpt'][gpt_ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aa08f4e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10961/10961 [00:00<00:00, 101044.13it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(len(test_data))):\n",
    "    if test_data.loc[i, 'question_id'] == 0:\n",
    "        cur_ques_id = test_data.loc[i, 'question_id']\n",
    "        gpt_ind = test_gpt.query('id == @cur_ques_id')['pred_gpt'].index[0]\n",
    "        test_data.loc[i, 'pred_gpt'] = test_gpt.query('id == @cur_ques_id')['pred_gpt'][gpt_ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a8830162",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10961/10961 [00:00<00:00, 19609.20it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(len(test_data))):\n",
    "    if test_data.loc[i, 'question_id'] in test_gpt_gpt4_rephrased_ids:\n",
    "        cur_ques_id = test_data.loc[i, 'question_id']\n",
    "        gpt_ind = test_gpt_gpt4_rephrased.query('id == @cur_ques_id')['pred_gpt'].index[0]\n",
    "        test_data.loc[i, 'pred_gpt'] = test_gpt_gpt4_rephrased.query('id == @cur_ques_id')['pred_gpt'][gpt_ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "24b5377b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10961/10961 [00:00<00:00, 30438.24it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(len(test_data))):\n",
    "    if test_data.loc[i, 'question_id'] in test_gpt_gpt4_answerdesc_ids:\n",
    "        cur_ques_id = test_data.loc[i, 'question_id']\n",
    "        gpt_ind = test_gpt_gpt4_answerdesc.query('id == @cur_ques_id')['pred_gpt'].index[0]\n",
    "        test_data.loc[i, 'pred_gpt'] = test_gpt_gpt4_answerdesc.query('id == @cur_ques_id')['pred_gpt'][gpt_ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cfd221c6-4a39-4da1-8dab-dda923868552",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10961/10961 [00:00<00:00, 45765.58it/s]\n"
     ]
    }
   ],
   "source": [
    "test_gpt_gpt4_konstruktor_ids = [366.0, 379.0, 466.0, 536.0, 758.0, 792.0, 808.0, 811.0, 825.0]\n",
    "\n",
    "for i in tqdm(range(len(test_data))):\n",
    "    if test_data.loc[i, 'question_id'] in test_gpt_gpt4_konstruktor_ids:\n",
    "        cur_ques_id = test_data.loc[i, 'question_id']\n",
    "        gpt_ind = test_konstruktor.query('id == @cur_ques_id')['pred_gpt'].index[0]\n",
    "        test_data.loc[i, 'pred_gpt'] = test_konstruktor.query('id == @cur_ques_id')['pred_gpt'][gpt_ind]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1fe8b8e-3335-4be5-aed8-b92e59276e91",
   "metadata": {},
   "source": [
    "### Split by the \"answer is\" text pattern"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3291e150-66a6-493c-9b43-9990873c395c",
   "metadata": {},
   "source": [
    "##### First check multiple answers, then singular (we want to split multiple by comma, but comma can also be a part of a singular answer)\n",
    "##### 1) Do it without stripping dot in the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0a027e64-8325-4e12-965c-28b9d658ad2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10961/10961 [00:00<00:00, 11089.27it/s]\n"
     ]
    }
   ],
   "source": [
    "for j in tqdm(range(len(test_data))):\n",
    "    if 'answer is:' in test_data.loc[j, 'pred_gpt']:\n",
    "        test_data.loc[j, 'gpt_answer'] = test_data.loc[j, 'pred_gpt'].split('answer is: ')[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0d188ac9-f6a4-44aa-9e09-bc8afea7d92f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10961/10961 [00:00<00:00, 13050.44it/s]\n"
     ]
    }
   ],
   "source": [
    "for j in tqdm(range(len(test_data))):\n",
    "    test_data.loc[j, 'gpt_answer'] = test_data.loc[j, 'gpt_answer'].replace('**', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "83582b17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10961/10961 [00:00<00:00, 31125.23it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(len(test_data))):\n",
    "    answer_ents = test_data.loc[i, 'gpt_answer'].split(', ')\n",
    "    for k in range(len(answer_ents)):\n",
    "        cur_answer_ent = answer_ents[k]\n",
    "        if pd.notna(test_data.loc[i, 'gpt_answer']) and \\\n",
    "        test_data.loc[i, 'answerEntity'].lower() == cur_answer_ent.lower() :\n",
    "            test_data.loc[i, 'prediction'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f93f1b5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10961/10961 [00:00<00:00, 31959.05it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(len(test_data))):\n",
    "    if pd.isna(test_data.loc[i, 'prediction']):\n",
    "        if pd.notna(test_data.loc[i, 'gpt_answer']) and \\\n",
    "        test_data.loc[i, 'answerEntity'].lower() == test_data.loc[i, 'gpt_answer'].lower() :\n",
    "            test_data.loc[i, 'prediction'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540ed698-29ad-47b1-8e75-86089c64f40d",
   "metadata": {},
   "source": [
    "#### 2) Do the same but with stripping dot in the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b6c01c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(test_data)):\n",
    "    if pd.notna(test_data.loc[i, 'gpt_answer']) and test_data.loc[i, 'gpt_answer'][-1] == '.':\n",
    "        test_data.loc[i, 'gpt_answer'] = test_data.loc[i, 'gpt_answer'][:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bce48b0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10961/10961 [00:00<00:00, 36223.35it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(len(test_data))):\n",
    "    if pd.isna(test_data.loc[i, 'prediction']):\n",
    "        answer_ents = test_data.loc[i, 'gpt_answer'].split(', ')\n",
    "        for k in range(len(answer_ents)):\n",
    "            cur_answer_ent = answer_ents[k]\n",
    "            if test_data.loc[i, 'answerEntity'].lower() == cur_answer_ent.lower() :\n",
    "                test_data.loc[i, 'prediction'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "be4cc29c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10961/10961 [00:00<00:00, 44516.26it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(len(test_data))):\n",
    "    if pd.isna(test_data.loc[i, 'prediction']):\n",
    "        if test_data.loc[i, 'answerEntity'].lower() == test_data.loc[i, 'gpt_answer'] :\n",
    "            test_data.loc[i, 'prediction'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1b82716f-ce2b-4594-9e06-2e50cea2fe7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ques_ids = set()\n",
    "for i in range(len(test_data)):\n",
    "    if test_data.loc[i, 'prediction'] == 1:\n",
    "        ques_ids.add(test_data.loc[i, 'question_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "11ccaf35-539a-4017-919f-d1b20a2ee10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ques_ids = list(ques_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "608d50e4-2dea-49a5-8048-645ffc6e2148",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(test_data)):\n",
    "    if pd.isna(test_data.loc[i, 'prediction']):\n",
    "        test_data.loc[i, 'prediction'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bdf1407a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data[\"prediction\"] = pd.to_numeric(test_data[\"prediction\"], downcast='integer')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0f5d82-2dd7-41aa-89fb-dc769401af17",
   "metadata": {},
   "source": [
    "### Check amount of questions with 0 matched answers, 1 answer and more than 1 answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3e3e8fe2-8100-4502-bcab-69a2b55fa522",
   "metadata": {},
   "outputs": [],
   "source": [
    "ques_ids = test_data.question_id.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "17c1f8bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero 31\n"
     ]
    }
   ],
   "source": [
    "nonmatched_answers = []\n",
    "count = 0\n",
    "for i in range(1000):\n",
    "    cur_ques_id = ques_ids[i]\n",
    "    if test_data.query('question_id == @cur_ques_id ').prediction.sum() == 0:\n",
    "        nonmatched_answers.append(cur_ques_id)\n",
    "        count += 1\n",
    "print('Zero', count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c20d7f45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One 803\n"
     ]
    }
   ],
   "source": [
    "nonmatched_answers = []\n",
    "count = 0\n",
    "for i in range(1000):\n",
    "    cur_ques_id = ques_ids[i]\n",
    "    if test_data.query('question_id == @cur_ques_id ').prediction.sum() == 1:\n",
    "        nonmatched_answers.append(cur_ques_id)\n",
    "        count += 1\n",
    "print('One', count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "590f7667",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "More than 1 166\n"
     ]
    }
   ],
   "source": [
    "nonmatched_answers = []\n",
    "count = 0\n",
    "for i in range(1000):\n",
    "    cur_ques_id = ques_ids[i]\n",
    "    if test_data.query('question_id == @cur_ques_id ').prediction.sum() > 1:\n",
    "        nonmatched_answers.append(cur_ques_id)\n",
    "        count += 1\n",
    "print('More than 1', count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02135507",
   "metadata": {},
   "source": [
    "### Solve the problem of zero answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "36dc89ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "ques_ids = list(test_data.question_id.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b70f5018",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31\n"
     ]
    }
   ],
   "source": [
    "nonmatched_answers = []\n",
    "count = 0\n",
    "for i in range(1000):\n",
    "    cur_ques_id = ques_ids[i]\n",
    "    if test_data.query('question_id == @cur_ques_id').prediction.sum() == 0:\n",
    "        nonmatched_answers.append(cur_ques_id)\n",
    "        count += 1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "497498e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10961/10961 [00:00<00:00, 99018.01it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jerry Rice, Jr Jerry Rice\n",
      "Battle of Verdun The Battle of Verdun\n",
      "Battle of Verdun The Battle of Verdun\n",
      "True Detective, season 1 True Detective, season 1\n",
      "Humphreys Peak Humphrey's Peak\n",
      "McFarland, USA McFarland, USA\n",
      "Mongol Empire The Mongol Empire\n",
      "George H. W. Bush George H.W. Bush\n",
      "George W. Bush George H.W. Bush\n",
      "Robert Gascoyne-Cecil, 3rd Marquess of Salisbury Robert Gascoyne-Cecil, 3rd Marquess of Salisbury\n",
      "Louis Antoine, Duke of Angoulême Louis Antoine, Duke of Angoulême\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(len(test_data))):\n",
    "    if test_data.loc[i, 'question_id'] in nonmatched_answers and pd.notna(test_data.loc[i, 'gpt_answer']):\n",
    "        if fuzz.ratio(test_data.loc[i, 'answerEntity'], test_data.loc[i, 'gpt_answer']) > 80 or\\\n",
    "                                                   levenshtein_distance(test_data.loc[i, 'answerEntity'], test_data.loc[i, 'gpt_answer'])== 1:\n",
    "            print(test_data.loc[i, 'answerEntity'], test_data.loc[i, 'gpt_answer'])\n",
    "            test_data.loc[i, 'prediction'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "64c345c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n"
     ]
    }
   ],
   "source": [
    "nonmatched_answers = []\n",
    "count = 0\n",
    "for i in range(1000):\n",
    "    cur_ques_id = ques_ids[i]\n",
    "    if test_data.query('question_id == @cur_ques_id').prediction.sum() == 0:\n",
    "        nonmatched_answers.append(cur_ques_id)\n",
    "        count += 1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0dfe8d80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10961/10961 [00:00<00:00, 66598.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the Discworld series by Terry Pratchett, which novel features the character Sam Vimes?\n",
      "Going Postal\n",
      "Given the options, none are primarily centered around Sam Vimes except for the brief involvement in \"Going Postal,\" but it's important to note that \"Going Postal\" and \"Terry Pratchett's Going Postal\" refer to the same story, with the latter being the adaptation's name. So, if we must choose, \"Terry Pratchett's Going Postal\" is the closest match due to its direct connection to the Discworld series, albeit not as a Vimes-centric narrative\n",
      "----------\n",
      "In the Discworld series by Terry Pratchett, which novel features the character Sam Vimes?\n",
      "Going Postal\n",
      "Given the options, none are primarily centered around Sam Vimes except for the brief involvement in \"Going Postal,\" but it's important to note that \"Going Postal\" and \"Terry Pratchett's Going Postal\" refer to the same story, with the latter being the adaptation's name. So, if we must choose, \"Terry Pratchett's Going Postal\" is the closest match due to its direct connection to the Discworld series, albeit not as a Vimes-centric narrative\n",
      "----------\n",
      "In the Discworld series by Terry Pratchett, which novel features the character Sam Vimes?\n",
      "Going Postal\n",
      "Given the options, none are primarily centered around Sam Vimes except for the brief involvement in \"Going Postal,\" but it's important to note that \"Going Postal\" and \"Terry Pratchett's Going Postal\" refer to the same story, with the latter being the adaptation's name. So, if we must choose, \"Terry Pratchett's Going Postal\" is the closest match due to its direct connection to the Discworld series, albeit not as a Vimes-centric narrative\n",
      "----------\n",
      "In the Discworld series by Terry Pratchett, which novel features the character Sam Vimes?\n",
      "Going Postal\n",
      "Given the options, none are primarily centered around Sam Vimes except for the brief involvement in \"Going Postal,\" but it's important to note that \"Going Postal\" and \"Terry Pratchett's Going Postal\" refer to the same story, with the latter being the adaptation's name. So, if we must choose, \"Terry Pratchett's Going Postal\" is the closest match due to its direct connection to the Discworld series, albeit not as a Vimes-centric narrative\n",
      "----------\n",
      "In the Discworld series by Terry Pratchett, which novel features the character Sam Vimes?\n",
      "Going Postal\n",
      "Given the options, none are primarily centered around Sam Vimes except for the brief involvement in \"Going Postal,\" but it's important to note that \"Going Postal\" and \"Terry Pratchett's Going Postal\" refer to the same story, with the latter being the adaptation's name. So, if we must choose, \"Terry Pratchett's Going Postal\" is the closest match due to its direct connection to the Discworld series, albeit not as a Vimes-centric narrative\n",
      "----------\n",
      "In the Discworld series by Terry Pratchett, which novel features the character Sam Vimes?\n",
      "Going Postal\n",
      "Given the options, none are primarily centered around Sam Vimes except for the brief involvement in \"Going Postal,\" but it's important to note that \"Going Postal\" and \"Terry Pratchett's Going Postal\" refer to the same story, with the latter being the adaptation's name. So, if we must choose, \"Terry Pratchett's Going Postal\" is the closest match due to its direct connection to the Discworld series, albeit not as a Vimes-centric narrative\n",
      "----------\n",
      "In the Discworld series by Terry Pratchett, which novel features the character Sam Vimes?\n",
      "Terry Pratchett's Going Postal\n",
      "Given the options, none are primarily centered around Sam Vimes except for the brief involvement in \"Going Postal,\" but it's important to note that \"Going Postal\" and \"Terry Pratchett's Going Postal\" refer to the same story, with the latter being the adaptation's name. So, if we must choose, \"Terry Pratchett's Going Postal\" is the closest match due to its direct connection to the Discworld series, albeit not as a Vimes-centric narrative\n",
      "----------\n",
      "In the Discworld series by Terry Pratchett, which novel features the character Sam Vimes?\n",
      "Terry Pratchett's Going Postal\n",
      "Given the options, none are primarily centered around Sam Vimes except for the brief involvement in \"Going Postal,\" but it's important to note that \"Going Postal\" and \"Terry Pratchett's Going Postal\" refer to the same story, with the latter being the adaptation's name. So, if we must choose, \"Terry Pratchett's Going Postal\" is the closest match due to its direct connection to the Discworld series, albeit not as a Vimes-centric narrative\n",
      "----------\n",
      "In the Discworld series by Terry Pratchett, which novel features the character Sam Vimes?\n",
      "Terry Pratchett's Going Postal\n",
      "Given the options, none are primarily centered around Sam Vimes except for the brief involvement in \"Going Postal,\" but it's important to note that \"Going Postal\" and \"Terry Pratchett's Going Postal\" refer to the same story, with the latter being the adaptation's name. So, if we must choose, \"Terry Pratchett's Going Postal\" is the closest match due to its direct connection to the Discworld series, albeit not as a Vimes-centric narrative\n",
      "----------\n",
      "In the Discworld series by Terry Pratchett, which novel features the character Sam Vimes?\n",
      "Terry Pratchett's Going Postal\n",
      "Given the options, none are primarily centered around Sam Vimes except for the brief involvement in \"Going Postal,\" but it's important to note that \"Going Postal\" and \"Terry Pratchett's Going Postal\" refer to the same story, with the latter being the adaptation's name. So, if we must choose, \"Terry Pratchett's Going Postal\" is the closest match due to its direct connection to the Discworld series, albeit not as a Vimes-centric narrative\n",
      "----------\n",
      "In the Discworld series by Terry Pratchett, which novel features the character Sam Vimes?\n",
      "Terry Pratchett's Going Postal\n",
      "Given the options, none are primarily centered around Sam Vimes except for the brief involvement in \"Going Postal,\" but it's important to note that \"Going Postal\" and \"Terry Pratchett's Going Postal\" refer to the same story, with the latter being the adaptation's name. So, if we must choose, \"Terry Pratchett's Going Postal\" is the closest match due to its direct connection to the Discworld series, albeit not as a Vimes-centric narrative\n",
      "----------\n",
      "In the Discworld series by Terry Pratchett, which novel features the character Sam Vimes?\n",
      "Terry Pratchett's Going Postal\n",
      "Given the options, none are primarily centered around Sam Vimes except for the brief involvement in \"Going Postal,\" but it's important to note that \"Going Postal\" and \"Terry Pratchett's Going Postal\" refer to the same story, with the latter being the adaptation's name. So, if we must choose, \"Terry Pratchett's Going Postal\" is the closest match due to its direct connection to the Discworld series, albeit not as a Vimes-centric narrative\n",
      "----------\n",
      "What had begun on 1 September 1939, when Nazi Germany, under Adolf Hitler, invaded Poland?\n",
      "The Second World War, taxation and the failure of community in colonial Ilesa metropolis, Western Nigeria\n",
      "The Second World War\n",
      "----------\n",
      "What is Tool's second album called?\n",
      "Aenima\n",
      "Ænima or Aenima\n",
      "----------\n",
      "What is Tool's second album called?\n",
      "Ænima\n",
      "Ænima or Aenima\n",
      "----------\n",
      "What is the first song on the album Blurryface?\n",
      "Blurryface\n",
      "This question seems to have an error as none of the listed options correctly identify the first song on \"Blurryface\" by Twenty One Pilots\n",
      "----------\n",
      "What team was third to last place in the 2020 MLB standings?\n",
      "Arizona Diamondbacks\n",
      "It seems I've reached a point where without the exact records, determining the answer with confidence between the provided options is not precise. However, given the context, we might lean more towards the Arizona Diamondbacks or the Boston Red Sox. Since the question allows for one answer and given the widespread acknowledgment of the Boston Red Sox's struggles, it might be safer to lean towards them, although this selection is made with the admission of uncertainty without the exact standings for final verification\n",
      "----------\n",
      "What team was third to last place in the 2020 MLB standings?\n",
      "Arizona Diamondbacks\n",
      "It seems I've reached a point where without the exact records, determining the answer with confidence between the provided options is not precise. However, given the context, we might lean more towards the Arizona Diamondbacks or the Boston Red Sox. Since the question allows for one answer and given the widespread acknowledgment of the Boston Red Sox's struggles, it might be safer to lean towards them, although this selection is made with the admission of uncertainty without the exact standings for final verification\n",
      "----------\n",
      "What team was third to last place in the 2020 MLB standings?\n",
      "Arizona Diamondbacks\n",
      "It seems I've reached a point where without the exact records, determining the answer with confidence between the provided options is not precise. However, given the context, we might lean more towards the Arizona Diamondbacks or the Boston Red Sox. Since the question allows for one answer and given the widespread acknowledgment of the Boston Red Sox's struggles, it might be safer to lean towards them, although this selection is made with the admission of uncertainty without the exact standings for final verification\n",
      "----------\n",
      "What team was third to last place in the 2020 MLB standings?\n",
      "Arizona Diamondbacks\n",
      "It seems I've reached a point where without the exact records, determining the answer with confidence between the provided options is not precise. However, given the context, we might lean more towards the Arizona Diamondbacks or the Boston Red Sox. Since the question allows for one answer and given the widespread acknowledgment of the Boston Red Sox's struggles, it might be safer to lean towards them, although this selection is made with the admission of uncertainty without the exact standings for final verification\n",
      "----------\n",
      "What team was third to last place in the 2020 MLB standings?\n",
      "Arizona Diamondbacks\n",
      "It seems I've reached a point where without the exact records, determining the answer with confidence between the provided options is not precise. However, given the context, we might lean more towards the Arizona Diamondbacks or the Boston Red Sox. Since the question allows for one answer and given the widespread acknowledgment of the Boston Red Sox's struggles, it might be safer to lean towards them, although this selection is made with the admission of uncertainty without the exact standings for final verification\n",
      "----------\n",
      "What team was third to last place in the 2020 MLB standings?\n",
      "Arizona Diamondbacks\n",
      "It seems I've reached a point where without the exact records, determining the answer with confidence between the provided options is not precise. However, given the context, we might lean more towards the Arizona Diamondbacks or the Boston Red Sox. Since the question allows for one answer and given the widespread acknowledgment of the Boston Red Sox's struggles, it might be safer to lean towards them, although this selection is made with the admission of uncertainty without the exact standings for final verification\n",
      "----------\n",
      "What team was third to last place in the 2020 MLB standings?\n",
      "Boston Red Sox\n",
      "It seems I've reached a point where without the exact records, determining the answer with confidence between the provided options is not precise. However, given the context, we might lean more towards the Arizona Diamondbacks or the Boston Red Sox. Since the question allows for one answer and given the widespread acknowledgment of the Boston Red Sox's struggles, it might be safer to lean towards them, although this selection is made with the admission of uncertainty without the exact standings for final verification\n",
      "----------\n",
      "What team was third to last place in the 2020 MLB standings?\n",
      "Boston Red Sox\n",
      "It seems I've reached a point where without the exact records, determining the answer with confidence between the provided options is not precise. However, given the context, we might lean more towards the Arizona Diamondbacks or the Boston Red Sox. Since the question allows for one answer and given the widespread acknowledgment of the Boston Red Sox's struggles, it might be safer to lean towards them, although this selection is made with the admission of uncertainty without the exact standings for final verification\n",
      "----------\n",
      "What team was third to last place in the 2020 MLB standings?\n",
      "Boston Red Sox\n",
      "It seems I've reached a point where without the exact records, determining the answer with confidence between the provided options is not precise. However, given the context, we might lean more towards the Arizona Diamondbacks or the Boston Red Sox. Since the question allows for one answer and given the widespread acknowledgment of the Boston Red Sox's struggles, it might be safer to lean towards them, although this selection is made with the admission of uncertainty without the exact standings for final verification\n",
      "----------\n",
      "What team was third to last place in the 2020 MLB standings?\n",
      "Boston Red Sox\n",
      "It seems I've reached a point where without the exact records, determining the answer with confidence between the provided options is not precise. However, given the context, we might lean more towards the Arizona Diamondbacks or the Boston Red Sox. Since the question allows for one answer and given the widespread acknowledgment of the Boston Red Sox's struggles, it might be safer to lean towards them, although this selection is made with the admission of uncertainty without the exact standings for final verification\n",
      "----------\n",
      "What team was third to last place in the 2020 MLB standings?\n",
      "Boston Red Sox\n",
      "It seems I've reached a point where without the exact records, determining the answer with confidence between the provided options is not precise. However, given the context, we might lean more towards the Arizona Diamondbacks or the Boston Red Sox. Since the question allows for one answer and given the widespread acknowledgment of the Boston Red Sox's struggles, it might be safer to lean towards them, although this selection is made with the admission of uncertainty without the exact standings for final verification\n",
      "----------\n",
      "What team was third to last place in the 2020 MLB standings?\n",
      "Boston Red Sox\n",
      "It seems I've reached a point where without the exact records, determining the answer with confidence between the provided options is not precise. However, given the context, we might lean more towards the Arizona Diamondbacks or the Boston Red Sox. Since the question allows for one answer and given the widespread acknowledgment of the Boston Red Sox's struggles, it might be safer to lean towards them, although this selection is made with the admission of uncertainty without the exact standings for final verification\n",
      "----------\n",
      "What was Buddy Guy's first album?\n",
      "Buddy Guy\n",
      "I made a mistake in reasoning, and there appears to be a misunderstanding since none of the options directly match the well-documented first album of Buddy Guy's career given the usual recognition of \"I Left My Blues in San Francisco\" or the collaborative \"Hoodoo Man Blues\" in the role played, with \"Hoodoo Man Blues\" being closer to the significant beginning impact in his recording career, but not solely under his name\n",
      "----------\n",
      "What was Buddy Guy's first album?\n",
      "Buddy Guy\n",
      "I made a mistake in reasoning, and there appears to be a misunderstanding since none of the options directly match the well-documented first album of Buddy Guy's career given the usual recognition of \"I Left My Blues in San Francisco\" or the collaborative \"Hoodoo Man Blues\" in the role played, with \"Hoodoo Man Blues\" being closer to the significant beginning impact in his recording career, but not solely under his name\n",
      "----------\n",
      "What was Buddy Guy's first album?\n",
      "Buddy Guy\n",
      "I made a mistake in reasoning, and there appears to be a misunderstanding since none of the options directly match the well-documented first album of Buddy Guy's career given the usual recognition of \"I Left My Blues in San Francisco\" or the collaborative \"Hoodoo Man Blues\" in the role played, with \"Hoodoo Man Blues\" being closer to the significant beginning impact in his recording career, but not solely under his name\n",
      "----------\n",
      "What was Buddy Guy's first album?\n",
      "Buddy Guy\n",
      "I made a mistake in reasoning, and there appears to be a misunderstanding since none of the options directly match the well-documented first album of Buddy Guy's career given the usual recognition of \"I Left My Blues in San Francisco\" or the collaborative \"Hoodoo Man Blues\" in the role played, with \"Hoodoo Man Blues\" being closer to the significant beginning impact in his recording career, but not solely under his name\n",
      "----------\n",
      "What was Buddy Guy's first album?\n",
      "Hoodoo Man Blues\n",
      "I made a mistake in reasoning, and there appears to be a misunderstanding since none of the options directly match the well-documented first album of Buddy Guy's career given the usual recognition of \"I Left My Blues in San Francisco\" or the collaborative \"Hoodoo Man Blues\" in the role played, with \"Hoodoo Man Blues\" being closer to the significant beginning impact in his recording career, but not solely under his name\n",
      "----------\n",
      "What was Buddy Guy's first album?\n",
      "Hoodoo Man Blues\n",
      "I made a mistake in reasoning, and there appears to be a misunderstanding since none of the options directly match the well-documented first album of Buddy Guy's career given the usual recognition of \"I Left My Blues in San Francisco\" or the collaborative \"Hoodoo Man Blues\" in the role played, with \"Hoodoo Man Blues\" being closer to the significant beginning impact in his recording career, but not solely under his name\n",
      "----------\n",
      "What was Buddy Guy's first album?\n",
      "Hoodoo Man Blues\n",
      "I made a mistake in reasoning, and there appears to be a misunderstanding since none of the options directly match the well-documented first album of Buddy Guy's career given the usual recognition of \"I Left My Blues in San Francisco\" or the collaborative \"Hoodoo Man Blues\" in the role played, with \"Hoodoo Man Blues\" being closer to the significant beginning impact in his recording career, but not solely under his name\n",
      "----------\n",
      "What was Buddy Guy's first album?\n",
      "Hoodoo Man Blues\n",
      "I made a mistake in reasoning, and there appears to be a misunderstanding since none of the options directly match the well-documented first album of Buddy Guy's career given the usual recognition of \"I Left My Blues in San Francisco\" or the collaborative \"Hoodoo Man Blues\" in the role played, with \"Hoodoo Man Blues\" being closer to the significant beginning impact in his recording career, but not solely under his name\n",
      "----------\n",
      "Where is that actor that plays Peter Parker in the MCU from?\n",
      "England\n",
      "England or London\n",
      "----------\n",
      "Where is that actor that plays Peter Parker in the MCU from?\n",
      "London\n",
      "England or London\n",
      "----------\n",
      "Which artist won the Grammy Award for Best Pop Vocal Album in 2006 and 2013?\n",
      "Adele\n",
      "Given the confusion and error in mapping the achievements properly to the years and artists from the provided list, no directly correct answer was provided based on the flawed approach to resolving the winners for both specified years in the question context. Therefore, I must conclude my analysis was incorrect, and without direct access to confirm which artist among those listed indeed meets the criteria of winning in both 2006 and 2013 within the confines of the question, my attempted logic has reached an impasse. \n",
      "\n",
      "Revisiting the list and focusing strictly on known high-profile wins and acknowledgments during these times, a correction in the reasoning process would lean towards recognizing that my approach to solving has overlooked the correct matching based on the provided list, reflecting a need for more precise recall of Grammy Award winners in those specific years. Without asserting an inaccurate confirmation and based on the guidelines provided for creating this response, I recommend considering the performance and popularity of artists like Adele around these times but also highlight the necessity of directly verifying such specifics to accurately match the Grammy win years as requested\n",
      "----------\n",
      "Which artist won the Grammy Award for Best Pop Vocal Album in 2006 and 2013?\n",
      "Adele\n",
      "Given the confusion and error in mapping the achievements properly to the years and artists from the provided list, no directly correct answer was provided based on the flawed approach to resolving the winners for both specified years in the question context. Therefore, I must conclude my analysis was incorrect, and without direct access to confirm which artist among those listed indeed meets the criteria of winning in both 2006 and 2013 within the confines of the question, my attempted logic has reached an impasse. \n",
      "\n",
      "Revisiting the list and focusing strictly on known high-profile wins and acknowledgments during these times, a correction in the reasoning process would lean towards recognizing that my approach to solving has overlooked the correct matching based on the provided list, reflecting a need for more precise recall of Grammy Award winners in those specific years. Without asserting an inaccurate confirmation and based on the guidelines provided for creating this response, I recommend considering the performance and popularity of artists like Adele around these times but also highlight the necessity of directly verifying such specifics to accurately match the Grammy win years as requested\n",
      "----------\n",
      "Which artist won the Grammy Award for Best Pop Vocal Album in 2006 and 2013?\n",
      "Adele\n",
      "Given the confusion and error in mapping the achievements properly to the years and artists from the provided list, no directly correct answer was provided based on the flawed approach to resolving the winners for both specified years in the question context. Therefore, I must conclude my analysis was incorrect, and without direct access to confirm which artist among those listed indeed meets the criteria of winning in both 2006 and 2013 within the confines of the question, my attempted logic has reached an impasse. \n",
      "\n",
      "Revisiting the list and focusing strictly on known high-profile wins and acknowledgments during these times, a correction in the reasoning process would lean towards recognizing that my approach to solving has overlooked the correct matching based on the provided list, reflecting a need for more precise recall of Grammy Award winners in those specific years. Without asserting an inaccurate confirmation and based on the guidelines provided for creating this response, I recommend considering the performance and popularity of artists like Adele around these times but also highlight the necessity of directly verifying such specifics to accurately match the Grammy win years as requested\n",
      "----------\n",
      "Which artist won the Grammy Award for Best Pop Vocal Album in 2006 and 2013?\n",
      "Adele\n",
      "Given the confusion and error in mapping the achievements properly to the years and artists from the provided list, no directly correct answer was provided based on the flawed approach to resolving the winners for both specified years in the question context. Therefore, I must conclude my analysis was incorrect, and without direct access to confirm which artist among those listed indeed meets the criteria of winning in both 2006 and 2013 within the confines of the question, my attempted logic has reached an impasse. \n",
      "\n",
      "Revisiting the list and focusing strictly on known high-profile wins and acknowledgments during these times, a correction in the reasoning process would lean towards recognizing that my approach to solving has overlooked the correct matching based on the provided list, reflecting a need for more precise recall of Grammy Award winners in those specific years. Without asserting an inaccurate confirmation and based on the guidelines provided for creating this response, I recommend considering the performance and popularity of artists like Adele around these times but also highlight the necessity of directly verifying such specifics to accurately match the Grammy win years as requested\n",
      "----------\n",
      "Which artist won the Grammy Award for Best Pop Vocal Album in 2006 and 2013?\n",
      "Adele\n",
      "Given the confusion and error in mapping the achievements properly to the years and artists from the provided list, no directly correct answer was provided based on the flawed approach to resolving the winners for both specified years in the question context. Therefore, I must conclude my analysis was incorrect, and without direct access to confirm which artist among those listed indeed meets the criteria of winning in both 2006 and 2013 within the confines of the question, my attempted logic has reached an impasse. \n",
      "\n",
      "Revisiting the list and focusing strictly on known high-profile wins and acknowledgments during these times, a correction in the reasoning process would lean towards recognizing that my approach to solving has overlooked the correct matching based on the provided list, reflecting a need for more precise recall of Grammy Award winners in those specific years. Without asserting an inaccurate confirmation and based on the guidelines provided for creating this response, I recommend considering the performance and popularity of artists like Adele around these times but also highlight the necessity of directly verifying such specifics to accurately match the Grammy win years as requested\n",
      "----------\n",
      "Which artist won the Grammy Award for Best Pop Vocal Album in 2006 and 2013?\n",
      "Adele\n",
      "Given the confusion and error in mapping the achievements properly to the years and artists from the provided list, no directly correct answer was provided based on the flawed approach to resolving the winners for both specified years in the question context. Therefore, I must conclude my analysis was incorrect, and without direct access to confirm which artist among those listed indeed meets the criteria of winning in both 2006 and 2013 within the confines of the question, my attempted logic has reached an impasse. \n",
      "\n",
      "Revisiting the list and focusing strictly on known high-profile wins and acknowledgments during these times, a correction in the reasoning process would lean towards recognizing that my approach to solving has overlooked the correct matching based on the provided list, reflecting a need for more precise recall of Grammy Award winners in those specific years. Without asserting an inaccurate confirmation and based on the guidelines provided for creating this response, I recommend considering the performance and popularity of artists like Adele around these times but also highlight the necessity of directly verifying such specifics to accurately match the Grammy win years as requested\n",
      "----------\n",
      "Which artist won the Grammy Award for Best Pop Vocal Album in 2006 and 2013?\n",
      "Adele\n",
      "Given the confusion and error in mapping the achievements properly to the years and artists from the provided list, no directly correct answer was provided based on the flawed approach to resolving the winners for both specified years in the question context. Therefore, I must conclude my analysis was incorrect, and without direct access to confirm which artist among those listed indeed meets the criteria of winning in both 2006 and 2013 within the confines of the question, my attempted logic has reached an impasse. \n",
      "\n",
      "Revisiting the list and focusing strictly on known high-profile wins and acknowledgments during these times, a correction in the reasoning process would lean towards recognizing that my approach to solving has overlooked the correct matching based on the provided list, reflecting a need for more precise recall of Grammy Award winners in those specific years. Without asserting an inaccurate confirmation and based on the guidelines provided for creating this response, I recommend considering the performance and popularity of artists like Adele around these times but also highlight the necessity of directly verifying such specifics to accurately match the Grammy win years as requested\n",
      "----------\n",
      "Which artist won the Grammy Award for Best Pop Vocal Album in 2006 and 2013?\n",
      "Adele\n",
      "Given the confusion and error in mapping the achievements properly to the years and artists from the provided list, no directly correct answer was provided based on the flawed approach to resolving the winners for both specified years in the question context. Therefore, I must conclude my analysis was incorrect, and without direct access to confirm which artist among those listed indeed meets the criteria of winning in both 2006 and 2013 within the confines of the question, my attempted logic has reached an impasse. \n",
      "\n",
      "Revisiting the list and focusing strictly on known high-profile wins and acknowledgments during these times, a correction in the reasoning process would lean towards recognizing that my approach to solving has overlooked the correct matching based on the provided list, reflecting a need for more precise recall of Grammy Award winners in those specific years. Without asserting an inaccurate confirmation and based on the guidelines provided for creating this response, I recommend considering the performance and popularity of artists like Adele around these times but also highlight the necessity of directly verifying such specifics to accurately match the Grammy win years as requested\n",
      "----------\n",
      "Which city has larger population, Kyoto or Oslo?\n",
      "Kyoto\n",
      "Without specific population numbers, the intent seems to be for the user to identify the cities within the options, which are \"Kyoto\" and indirectly \"Oslo\" through \"The City of Oslo Art Collection.\" However, the direct question of which has a larger population cannot be accurately answered with the data provided\n",
      "----------\n",
      "Which city has larger population, Kyoto or Oslo?\n",
      "Kyoto\n",
      "Without specific population numbers, the intent seems to be for the user to identify the cities within the options, which are \"Kyoto\" and indirectly \"Oslo\" through \"The City of Oslo Art Collection.\" However, the direct question of which has a larger population cannot be accurately answered with the data provided\n",
      "----------\n",
      "Which city has larger population, Kyoto or Oslo?\n",
      "Kyoto\n",
      "Without specific population numbers, the intent seems to be for the user to identify the cities within the options, which are \"Kyoto\" and indirectly \"Oslo\" through \"The City of Oslo Art Collection.\" However, the direct question of which has a larger population cannot be accurately answered with the data provided\n",
      "----------\n",
      "Which city has larger population, Kyoto or Oslo?\n",
      "Kyoto\n",
      "Without specific population numbers, the intent seems to be for the user to identify the cities within the options, which are \"Kyoto\" and indirectly \"Oslo\" through \"The City of Oslo Art Collection.\" However, the direct question of which has a larger population cannot be accurately answered with the data provided\n",
      "----------\n",
      "Which city has larger population, Kyoto or Oslo?\n",
      "The City of Oslo Art Collection\n",
      "Without specific population numbers, the intent seems to be for the user to identify the cities within the options, which are \"Kyoto\" and indirectly \"Oslo\" through \"The City of Oslo Art Collection.\" However, the direct question of which has a larger population cannot be accurately answered with the data provided\n",
      "----------\n",
      "Which city has larger population, Kyoto or Oslo?\n",
      "The City of Oslo Art Collection\n",
      "Without specific population numbers, the intent seems to be for the user to identify the cities within the options, which are \"Kyoto\" and indirectly \"Oslo\" through \"The City of Oslo Art Collection.\" However, the direct question of which has a larger population cannot be accurately answered with the data provided\n",
      "----------\n",
      "Which city has larger population, Kyoto or Oslo?\n",
      "The City of Oslo Art Collection\n",
      "Without specific population numbers, the intent seems to be for the user to identify the cities within the options, which are \"Kyoto\" and indirectly \"Oslo\" through \"The City of Oslo Art Collection.\" However, the direct question of which has a larger population cannot be accurately answered with the data provided\n",
      "----------\n",
      "Which city has larger population, Kyoto or Oslo?\n",
      "The City of Oslo Art Collection\n",
      "Without specific population numbers, the intent seems to be for the user to identify the cities within the options, which are \"Kyoto\" and indirectly \"Oslo\" through \"The City of Oslo Art Collection.\" However, the direct question of which has a larger population cannot be accurately answered with the data provided\n",
      "----------\n",
      "Which countries hosted ICC world cup in 1992?\n",
      "Australia\n",
      "There is no precise match, but based on the requirement to choose from the given options, the expected answer related to the 1992 Cricket World Cup host nations would be both New Zealand and Australia, even though an exact match isn't provided among the choices\n",
      "----------\n",
      "Which countries hosted ICC world cup in 1992?\n",
      "Australia\n",
      "There is no precise match, but based on the requirement to choose from the given options, the expected answer related to the 1992 Cricket World Cup host nations would be both New Zealand and Australia, even though an exact match isn't provided among the choices\n",
      "----------\n",
      "Which countries hosted ICC world cup in 1992?\n",
      "Australia\n",
      "There is no precise match, but based on the requirement to choose from the given options, the expected answer related to the 1992 Cricket World Cup host nations would be both New Zealand and Australia, even though an exact match isn't provided among the choices\n",
      "----------\n",
      "Which countries hosted ICC world cup in 1992?\n",
      "Australia\n",
      "There is no precise match, but based on the requirement to choose from the given options, the expected answer related to the 1992 Cricket World Cup host nations would be both New Zealand and Australia, even though an exact match isn't provided among the choices\n",
      "----------\n",
      "Which countries hosted ICC world cup in 1992?\n",
      "New Zealand\n",
      "There is no precise match, but based on the requirement to choose from the given options, the expected answer related to the 1992 Cricket World Cup host nations would be both New Zealand and Australia, even though an exact match isn't provided among the choices\n",
      "----------\n",
      "Which countries hosted ICC world cup in 1992?\n",
      "New Zealand\n",
      "There is no precise match, but based on the requirement to choose from the given options, the expected answer related to the 1992 Cricket World Cup host nations would be both New Zealand and Australia, even though an exact match isn't provided among the choices\n",
      "----------\n",
      "Which countries hosted ICC world cup in 1992?\n",
      "New Zealand\n",
      "There is no precise match, but based on the requirement to choose from the given options, the expected answer related to the 1992 Cricket World Cup host nations would be both New Zealand and Australia, even though an exact match isn't provided among the choices\n",
      "----------\n",
      "Which countries hosted ICC world cup in 1992?\n",
      "New Zealand\n",
      "There is no precise match, but based on the requirement to choose from the given options, the expected answer related to the 1992 Cricket World Cup host nations would be both New Zealand and Australia, even though an exact match isn't provided among the choices\n",
      "----------\n",
      "Which of King Solomon's wives was never known by another name?\n",
      "Na'ama\n",
      "Na'ama/Naamah\n",
      "----------\n",
      "Which of King Solomon's wives was never known by another name?\n",
      "Naamah\n",
      "Na'ama/Naamah\n",
      "----------\n",
      "Which war did not end in a hundred years?\n",
      "Hundred Years' War\n",
      "All except the Hundred Years' War did not end in a hundred years\n",
      "----------\n",
      "Who launched the third Roman invasion of Britain?\n",
      "Claudius\n",
      "The question seems to be based on a misunderstanding or mislabeling of the historical invasions of Britain as none of the options provided are known for a \"third\" invasion, but Claudius would be the closest in context, despite the mismatch with the question's phrasing\n",
      "----------\n",
      "Who launched the third Roman invasion of Britain?\n",
      "Claudius\n",
      "The question seems to be based on a misunderstanding or mislabeling of the historical invasions of Britain as none of the options provided are known for a \"third\" invasion, but Claudius would be the closest in context, despite the mismatch with the question's phrasing\n",
      "----------\n",
      "Who launched the third Roman invasion of Britain?\n",
      "Claudius\n",
      "The question seems to be based on a misunderstanding or mislabeling of the historical invasions of Britain as none of the options provided are known for a \"third\" invasion, but Claudius would be the closest in context, despite the mismatch with the question's phrasing\n",
      "----------\n",
      "Who ran 2019 Japanese House of Councillors election and graduated from Kobe Universit?\n",
      "Hiroyuki Yamamoto\n",
      "without having direct access to specific verification data here, my original logic process can't identify which one between Hiroyuki Yamamoto and Yoshifumi Hamano better fits the criteria based on the information provided\n",
      "----------\n",
      "Who ran 2019 Japanese House of Councillors election and graduated from Kobe Universit?\n",
      "Hiroyuki Yamamoto\n",
      "without having direct access to specific verification data here, my original logic process can't identify which one between Hiroyuki Yamamoto and Yoshifumi Hamano better fits the criteria based on the information provided\n",
      "----------\n",
      "Who ran 2019 Japanese House of Councillors election and graduated from Kobe Universit?\n",
      "Yoshifumi Hamano\n",
      "without having direct access to specific verification data here, my original logic process can't identify which one between Hiroyuki Yamamoto and Yoshifumi Hamano better fits the criteria based on the information provided\n",
      "----------\n",
      "Who ran 2019 Japanese House of Councillors election and graduated from Kobe Universit?\n",
      "Yoshifumi Hamano\n",
      "without having direct access to specific verification data here, my original logic process can't identify which one between Hiroyuki Yamamoto and Yoshifumi Hamano better fits the criteria based on the information provided\n",
      "----------\n",
      "Who was a former Green Beret and 46th governor of Florida?\n",
      "Michael Waltz\n",
      "Upon review, none of the provided options accurately matches the distinction of being both a former Green Beret and the 46th governor of Florida, indicating an error in my analysis or a misunderstanding of the roles held by the individuals named. Considering the closest match based on the criteria might have been intended as Michael Waltz for his military service, but with the significant note that he was not the governor, leading to a conclusion that the question's accurate answer may not be represented within the options based on the latest available knowledge or an error in the premise of the question\n",
      "----------\n",
      "Who was a former Green Beret and 46th governor of Florida?\n",
      "Michael Waltz\n",
      "Upon review, none of the provided options accurately matches the distinction of being both a former Green Beret and the 46th governor of Florida, indicating an error in my analysis or a misunderstanding of the roles held by the individuals named. Considering the closest match based on the criteria might have been intended as Michael Waltz for his military service, but with the significant note that he was not the governor, leading to a conclusion that the question's accurate answer may not be represented within the options based on the latest available knowledge or an error in the premise of the question\n",
      "----------\n",
      "Who was a former Green Beret and 46th governor of Florida?\n",
      "Michael Waltz\n",
      "Upon review, none of the provided options accurately matches the distinction of being both a former Green Beret and the 46th governor of Florida, indicating an error in my analysis or a misunderstanding of the roles held by the individuals named. Considering the closest match based on the criteria might have been intended as Michael Waltz for his military service, but with the significant note that he was not the governor, leading to a conclusion that the question's accurate answer may not be represented within the options based on the latest available knowledge or an error in the premise of the question\n",
      "----------\n",
      "Who was a former Green Beret and 46th governor of Florida?\n",
      "Michael Waltz\n",
      "Upon review, none of the provided options accurately matches the distinction of being both a former Green Beret and the 46th governor of Florida, indicating an error in my analysis or a misunderstanding of the roles held by the individuals named. Considering the closest match based on the criteria might have been intended as Michael Waltz for his military service, but with the significant note that he was not the governor, leading to a conclusion that the question's accurate answer may not be represented within the options based on the latest available knowledge or an error in the premise of the question\n",
      "----------\n",
      "Who was a former Green Beret and 46th governor of Florida?\n",
      "Michael Waltz\n",
      "Upon review, none of the provided options accurately matches the distinction of being both a former Green Beret and the 46th governor of Florida, indicating an error in my analysis or a misunderstanding of the roles held by the individuals named. Considering the closest match based on the criteria might have been intended as Michael Waltz for his military service, but with the significant note that he was not the governor, leading to a conclusion that the question's accurate answer may not be represented within the options based on the latest available knowledge or an error in the premise of the question\n",
      "----------\n",
      "Who was the last Chinese president?\n",
      "Chinese Communist Party general secretary  Hu Jintao singing Moscow Nights\n",
      "Hu Jintao\n",
      "----------\n",
      "Who was the last Chinese president?\n",
      "Hu Jintao's removal from the 20th National Congress of the Chinese Communist Party\n",
      "Hu Jintao\n",
      "----------\n",
      "Who won the 2014 NBA Finals?\n",
      "San Antonio\n",
      "the correct option isn't listed, but the factual answer is the San Antonio Spurs won the 2014 NBA Finals\n",
      "----------\n",
      "Who won the 2014 NBA Finals?\n",
      "San Antonio\n",
      "the correct option isn't listed, but the factual answer is the San Antonio Spurs won the 2014 NBA Finals\n",
      "----------\n",
      "Who won the 2014 NBA Finals?\n",
      "San Antonio\n",
      "the correct option isn't listed, but the factual answer is the San Antonio Spurs won the 2014 NBA Finals\n",
      "----------\n",
      "Who won the 2014 NBA Finals?\n",
      "San Antonio\n",
      "the correct option isn't listed, but the factual answer is the San Antonio Spurs won the 2014 NBA Finals\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "check_ids = set()\n",
    "for i in tqdm(range(len(test_data))):\n",
    "        if test_data.loc[i, 'question_id'] in nonmatched_answers and pd.notna(test_data.loc[i, 'gpt_answer']):\n",
    "            answer_ents = test_data.loc[i, 'gpt_answer'].split(', ')\n",
    "            for k in range(len(answer_ents)):\n",
    "                cur_answer_ent = answer_ents[k]\n",
    "                if test_data.loc[i, 'gpt_answer'].lower() in test_data.loc[i, 'answerEntity'].lower() or test_data.loc[i, 'answerEntity'].lower() in test_data.loc[i, 'gpt_answer'].lower():\n",
    "                    check_ids.add(test_data.loc[i, 'question_id'])\n",
    "                    print(test_data.loc[i, 'question'])\n",
    "                    print(test_data.loc[i, 'answerEntity'])\n",
    "                    print(test_data.loc[i, 'gpt_answer'])\n",
    "                    print('-'*10)\n",
    "                    test_data.loc[i, 'prediction'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "af1a9673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "nonmatched_answers = []\n",
    "count = 0\n",
    "for i in range(1000):\n",
    "    cur_ques_id = ques_ids[i]\n",
    "    if test_data.query('question_id == @cur_ques_id').prediction.sum() == 0:\n",
    "        nonmatched_answers.append(cur_ques_id)\n",
    "        count += 1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552ab53c",
   "metadata": {},
   "source": [
    "### Fix numerous by amount of links"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446e539f-f37f-434e-99a1-4a578fd5b3cc",
   "metadata": {},
   "source": [
    "##### download answer descriptions of answer entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f5bda3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data2 = pd.read_csv('data/tsv/test_ans_desc.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9f9cc467",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['answer_desc'] = test_data2['answer_desc']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6f51da-8846-4cac-8d17-82c857fdc031",
   "metadata": {},
   "source": [
    "##### calculate amount of edges in the answer subgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "df093838",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10961/10961 [00:02<00:00, 4704.75it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(len(test_data))):\n",
    "    test_data.loc[i, 'n_links'] = len(ast.literal_eval(test_data.loc[i, 'graph'])['links'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e04bb4-6ea5-47bf-bf6e-d4f445177700",
   "metadata": {},
   "source": [
    "#### Fix comparative type of questions (it shoud be just one answer), in questions like \"how is older A or B\", the answer is exactly A or B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "883bf7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ques_comparative = []\n",
    "for i in range(len(ques_ids)):\n",
    "    cur_ques_id = ques_ids[i]\n",
    "    cur_ques = test_data.query('question_id == @cur_ques_id')['question'].unique()[0]\n",
    "    if \" or \" in cur_ques:\n",
    "        ques_comparative.append(cur_ques_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ee86fe26-3c9f-478f-b664-4dcfdbf7ac2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(ques_ids)):\n",
    "    cur_ques_id = ques_ids[i]\n",
    "    cur_ques = test_data.query('question_id == @cur_ques_id')['question'].unique()[0]\n",
    "    if \" or \" in cur_ques:\n",
    "        for elem in test_data.query('question_id == @cur_ques_id').reset_index(drop=True).loc[0,'questionEntityId'].split(', '):\n",
    "            if elem not in test_data.query('question_id == @cur_ques_id')['answerEntityId'].unique():\n",
    "                ques_comparative.remove(cur_ques_id)\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "bb6990b1-3bed-48ca-be81-6a9147ce39fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ques_comparative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "021c0480",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(ques_comparative)):\n",
    "    \n",
    "    cur_ques_id = ques_comparative[i]\n",
    "    test_subset = test_data.query('question_id == @cur_ques_id').reset_index(drop=True)\n",
    "    best_ind = 0\n",
    "    max_score = 0\n",
    "    for j in range(len(test_subset)):\n",
    "        if test_subset.loc[j, 'answerEntityId'] in test_subset.loc[j, 'questionEntityId']:\n",
    "            x = sentence_embedding(test_subset.loc[j, 'answerEntity'])\n",
    "            op = sentence_embedding(test_subset.loc[j, 'gpt_answer'])\n",
    "            cur_cos_sim = 1 - scipy.spatial.distance.cosine(x, op)\n",
    "            if cur_cos_sim > max_score:\n",
    "                max_score = cur_cos_sim \n",
    "                best_ind = j\n",
    "    if max_score > 0.5:\n",
    "        test_inds = list(test_data.query('question_id == @cur_ques_id').index)\n",
    "        test_data.loc[test_inds[best_ind], 'prediction'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a53bd7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(test_data)):\n",
    "    if test_data.loc[i, 'question_id'] in ques_comparative and test_data.loc[i, 'answerEntityId'] not in test_data.loc[i, 'questionEntityId'].split(', '):\n",
    "        test_data.loc[i, 'prediction'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d5681540",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "829.0\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(ques_ids)):\n",
    "    cur_ques_id = ques_ids[i]\n",
    "    cur_ques = test_data.query('question_id == @cur_ques_id')['question'].unique()[0]\n",
    "    if \" or \" in cur_ques and test_data.query('question_id == @cur_ques_id').prediction.sum() == 0:\n",
    "        print(cur_ques_id )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8bf7bf94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159\n"
     ]
    }
   ],
   "source": [
    "nonmatched_answers = []\n",
    "count = 0\n",
    "for i in range(1000):\n",
    "    cur_ques_id = ques_ids[i]\n",
    "    if test_data.query('question_id == @cur_ques_id ').prediction.sum() >1:\n",
    "        nonmatched_answers.append(cur_ques_id)\n",
    "        count += 1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17fbef4d-4659-4fd7-9086-d5765408eb25",
   "metadata": {},
   "source": [
    "#### If there are different multiple answers do not modify them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "93e963f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1000):\n",
    "    cur_ques_id = ques_ids[i]\n",
    "    if cur_ques_id in nonmatched_answers:\n",
    "        if len(test_data.query('question_id == @cur_ques_id').reset_index(drop=True).loc[0, 'gpt_answer'].split(', ')) == len(test_data.query('question_id == @cur_ques_id and prediction == 1').answerEntityId.unique()):\n",
    "            nonmatched_answers.remove(cur_ques_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "374dd153-a5c6-4113-9fff-ba7af0a1da3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "139"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nonmatched_answers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d39b50-b726-46e1-869b-69ade2371d9c",
   "metadata": {},
   "source": [
    "## The answers with identical labels could be modified in four ways:\n",
    "1) select the ones where the size of subgraph is minimal (section: fix numerous by amount of links)\n",
    "2) select the ones where the correct_path is maximal (section: fix numerous by correct_path)\n",
    "3) select the ones where the cosine similarity of embeddings between answer description and question is maximum (section: fix numerous by cosine similarity)\n",
    "4) always select the first one out of conflicting options"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f00170-319a-48ee-a7b6-04c658caa4ff",
   "metadata": {},
   "source": [
    "#### fix numerous by amount of links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e39b5ccc-ae4d-4c6b-90cf-b4b3e2cf8e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_remove = []\n",
    "for i in range(len(nonmatched_answers)):\n",
    "    cur_ques_id = nonmatched_answers[i]\n",
    "    test_ent = test_data.query('question_id == @cur_ques_id and prediction == 1').answerEntity.unique()\n",
    "    test_ent_upd = set([elem.lower() for elem in test_ent])\n",
    "    \n",
    "    if len(test_ent_upd) > 1:\n",
    "        for p in range(len(test_ent)):\n",
    "            cur_ent = test_ent[p]\n",
    "            min_links = test_data.query('question_id == @cur_ques_id and prediction == 1 and answerEntity == @cur_ent').n_links.min()\n",
    "            id_index = list(test_data.query('question_id == @cur_ques_id and prediction == 1 and answerEntity == @cur_ent and n_links == @min_links').index)\n",
    "            inds_all = list(test_data.query('question_id == @cur_ques_id and prediction == 1 and answerEntity == @cur_ent').index)\n",
    "            if len(id_index) == 1:\n",
    "                for j in range(len(inds_all)):\n",
    "                     test_data.loc[inds_all[j], 'prediction'] = 0\n",
    "                for q in range(len(id_index)):\n",
    "                    test_data.loc[id_index[q], 'prediction'] = 1\n",
    "                to_remove.append(cur_ques_id)\n",
    "    else:\n",
    "        min_links = test_data.query('question_id == @cur_ques_id and prediction == 1 ').n_links.min()\n",
    "        id_index = list(test_data.query('question_id == @cur_ques_id and prediction == 1 and n_links == @min_links').index)\n",
    "        inds_all = list(test_data.query('question_id == @cur_ques_id and prediction == 1').index)\n",
    "        if len(id_index) == 1:\n",
    "            for j in range(len(inds_all)):\n",
    "                 test_data.loc[inds_all[j], 'prediction'] = 0\n",
    "            for q in range(len(id_index)):\n",
    "                test_data.loc[id_index[q], 'prediction'] = 1\n",
    "            to_remove.append(cur_ques_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2c5f5afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_remove = list(set(to_remove))\n",
    "for q in range(len(to_remove)):\n",
    "    if to_remove[q] in nonmatched_answers:\n",
    "        nonmatched_answers.remove(to_remove[q])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "929d8bda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nonmatched_answers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04bb3159-5361-440b-bbb2-ecceb30cd102",
   "metadata": {},
   "source": [
    "#### fix numerous by correct_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1671,
   "id": "b6d6d487-2ff6-432f-a620-c5684d8634d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to_remove = []\n",
    "# for i in range(len(nonmatched_answers)):\n",
    "#     cur_ques_id = nonmatched_answers[i]\n",
    "#     test_ent = test_data.query('question_id == @cur_ques_id and prediction == 1').answerEntity.unique()\n",
    "#     test_ent_upd = set([elem.lower() for elem in test_ent])\n",
    "    \n",
    "#     if len(test_ent_upd) > 1:\n",
    "#         for p in range(len(test_ent)):\n",
    "#             cur_ent = test_ent[p]\n",
    "#             max_path = test_data.query('question_id == @cur_ques_id and prediction == 1 and answerEntity == @cur_ent').correct_path.max()\n",
    "#             id_index = list(test_data.query('question_id == @cur_ques_id and prediction == 1 and answerEntity == @cur_ent and correct_path == @max_path').index)\n",
    "#             inds_all = list(test_data.query('question_id == @cur_ques_id and prediction == 1 and answerEntity == @cur_ent').index)\n",
    "#             if len(id_index) == 1:\n",
    "#                 for j in range(len(inds_all)):\n",
    "#                      test_data.loc[inds_all[j], 'prediction'] = 0\n",
    "#                 for q in range(len(id_index)):\n",
    "#                     test_data.loc[id_index[q], 'prediction'] = 1\n",
    "#                 to_remove.append(cur_ques_id)\n",
    "#     else:\n",
    "#         max_path = test_data.query('question_id == @cur_ques_id and prediction == 1 ').correct_path.max()\n",
    "#         id_index = list(test_data.query('question_id == @cur_ques_id and prediction == 1 and correct_path == @max_path').index)\n",
    "#         inds_all = list(test_data.query('question_id == @cur_ques_id and prediction == 1').index)\n",
    "#         if len(id_index) == 1:\n",
    "#             for j in range(len(inds_all)):\n",
    "#                  test_data.loc[inds_all[j], 'prediction_nlinks'] = 0\n",
    "#             for q in range(len(id_index)):\n",
    "#                 test_data.loc[id_index[q], 'prediction_nlinks'] = 1\n",
    "#             to_remove.append(cur_ques_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1672,
   "id": "6f129c53-d732-4d22-aef7-7eb267a3fce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to_remove = list(set(to_remove))\n",
    "# for q in range(len(to_remove)):\n",
    "#     if to_remove[q] in nonmatched_answers:\n",
    "#         nonmatched_answers.remove(to_remove[q])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1673,
   "id": "0c57ecdc-6fff-4cbb-bd9f-ef38e767794f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 1673,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(nonmatched_answers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3227a89-c0da-4b16-ac0f-f329e840a9d6",
   "metadata": {},
   "source": [
    "#### fix numerous by cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 646,
   "id": "a5d654c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to_remove = []\n",
    "# for i in range(len(nonmatched_answers)):\n",
    "#     cur_ques_id = nonmatched_answers[i]\n",
    "#     test_ent = test_data.query('question_id == @cur_ques_id and prediction == 1').answerEntity.unique()\n",
    "#     for p in range(len(test_ent)):\n",
    "#         cur_ent = test_ent[p]\n",
    "#         test_subset = test_data.query('question_id == @cur_ques_id and prediction == 1 and answerEntity == @cur_ent').reset_index(drop=True)\n",
    "#         cur_question = test_subset.loc[0, 'question']\n",
    "#         cos_sim_best = 0\n",
    "#         best_ind = 0\n",
    "        \n",
    "#         for k in range(len(test_subset)):\n",
    "#             if pd.notna(test_subset.loc[k, 'answer_desc']):\n",
    "#                 x = sentence_embedding(cur_question)\n",
    "#                 op = sentence_embedding(test_subset.loc[k, 'answer_desc'])\n",
    "#                 cur_cos_sim = 1 - scipy.spatial.distance.cosine(x, op)\n",
    "#                 if cur_cos_sim > cos_sim_best:\n",
    "#                     cos_sim_best = cur_cos_sim\n",
    "#                     best_ind = k\n",
    "#         inds_init = list(test_data.query('question_id == @cur_ques_id and prediction == 1 and answerEntity == @cur_ent').index)\n",
    "#         for q in range(len(inds_init)):\n",
    "#             if q!=best_ind:\n",
    "#                 test_data.loc[inds_init[q], 'prediction']= 0\n",
    "#                 to_remove.append(cur_ques_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6406596-1af0-4d32-b8f2-202b495d9900",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to_remove = list(set(to_remove))\n",
    "# for q in range(len(to_remove)):\n",
    "#     if to_remove[q] in nonmatched_answers:\n",
    "#         nonmatched_answers.remove(to_remove[q])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b90a99f-0596-41e1-9fd6-bd87948e574e",
   "metadata": {},
   "source": [
    "#### Always select the first one out of conflicting options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b138447f-0a0d-4c10-95b3-9e25a55363f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_remove = []\n",
    "for i in range(len(nonmatched_answers)):\n",
    "    cur_ques_id = nonmatched_answers[i]\n",
    "    test_ent = test_data.query('question_id == @cur_ques_id and prediction == 1').answerEntity.unique()\n",
    "    for p in range(len(test_ent)):\n",
    "        cur_ent = test_ent[p]\n",
    "        test_subset = test_data.query('question_id == @cur_ques_id and prediction == 1 and answerEntity == @cur_ent').reset_index(drop=True)\n",
    "        cur_question = test_subset.loc[0, 'question']\n",
    "        cos_sim_best = 0\n",
    "        best_ind = 0\n",
    "        inds_init = list(test_data.query('question_id == @cur_ques_id and prediction == 1 and answerEntity == @cur_ent').index)\n",
    "        for q in range(len(inds_init)):\n",
    "            if q!=best_ind:\n",
    "                test_data.loc[inds_init[q], 'prediction']= 0\n",
    "                to_remove.append(cur_ques_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7ad3a8ba-c669-4f8e-8f49-9012e8587de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_remove = list(set(to_remove))\n",
    "for q in range(len(to_remove)):\n",
    "    if to_remove[q] in nonmatched_answers:\n",
    "        nonmatched_answers.remove(to_remove[q])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "361d1014-351e-4a41-bac9-5ad2ad3d280d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nonmatched_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "67f46e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.loc[5078, 'prediction'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ce6bd28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.loc[9086, 'prediction'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ebe4918",
   "metadata": {},
   "source": [
    "### Final checks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0feb3b97-7cb8-4041-883d-669205395664",
   "metadata": {},
   "source": [
    "#### before submission check the amount of questions for which we didn't find answers for the option and the questions with multiple answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6b95c289",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero 5\n"
     ]
    }
   ],
   "source": [
    "nonmatched_answers_zero = []\n",
    "count = 0\n",
    "for i in range(1000):\n",
    "    cur_ques_id = ques_ids[i]\n",
    "    if test_data.query('question_id == @cur_ques_id ').prediction.sum() == 0:\n",
    "        nonmatched_answers_zero.append(cur_ques_id)\n",
    "        count += 1\n",
    "print('Zero', count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e0c66c09-5028-4747-919a-0d4ea24a45be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "More than one 36\n"
     ]
    }
   ],
   "source": [
    "nonmatched_answers_more = []\n",
    "count = 0\n",
    "for i in range(1000):\n",
    "    cur_ques_id = ques_ids[i]\n",
    "    if test_data.query('question_id == @cur_ques_id ').prediction.sum() > 1:\n",
    "        nonmatched_answers_more.append(cur_ques_id)\n",
    "        count += 1\n",
    "print('More than one', count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045a395f-a780-45e7-8342-ebae72cdd037",
   "metadata": {},
   "source": [
    "#### submission file creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1831,
   "id": "bdf5d88b-9551-440b-83b4-05f8c255ea89",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data[['sample_id', 'prediction']].to_csv('data/csv/test_submission_posteval_fixed_v2_nlinks_first_newkonstruktor.tsv',\n",
    "                                              sep=\"\\t\",\n",
    "                                             index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
